---
timestamp: 'Thu Oct 16 2025 13:31:49 GMT-0400 (Eastern Daylight Time)'
parent: '[[..\20251016_133149.d1790cf5.md]]'
content_id: eea7fc5183db161e5dc1852fdba567d420e0bdefe53b3ff1e486d9404d7297a2
---

# ProgressTracking Concept

[20251015\_224640.92baf8e4](../context/design/concepts/ProgressTracking/implementation.md/20251015_224640.92baf8e4.md)

General best practices and the specific challenges related to context management:

1. **Critical Inconsistency in Iterative Context Application (LLM Challenge):**
   * **Observation:** While external file references (like `../context/...`) are effective for providing *initial* context, the AI demonstrated a challenge in consistently applying *new or iteratively refined context* to its *already generated output*.
   * **Example from PasswordAuthentication, ProgressTracking:** The AI *itself* introduced a requirement for usernames to be `>= 8` characters but later, in a subsequent step of the *same interaction*, generated an "interesting case" test with a 7-character username ("userBob"), forgetting its own recent recommendation. I forgot to include the background material relating to implementing concepts before asking the LLM to implement the ProgressTrackingConcept. It understandably created a class based on OOP principles. However, when I later added the implementing concepts doc as context to the top of the document, the LLM failed to incorporate this context and still outputted the same class.
   * **Lesson Regarding Context Injection:** This highlights that merely providing additional context after an initial generation doesn't guarantee a re-evaluation and modification of previous outputs. The expectation is that new context, even if provided iteratively, should trigger a re-assessment of the *entire* concept's implementation and associated tests to maintain consistency, especially when context is linked via external files. This emphasizes a need for more continuous and dynamic context awareness for robust iterative development.

[20251016\_095310.3a9e2088](../context/design/concepts/ProgressTracking/ProgressTrackingSpec.md/20251016_095310.3a9e2088.md)

Important lessons extracted from our interaction, with a focus on the `ProgressTracking Concept`:

1. **Ambiguous Query Specifications Lead to Undesired Exposure (LLM Interpretation):**
   When the `ProgressTracking` concept initially defined the `_getPlans` query with the effect "returns an array of all existing Plans belonging to user," but `Plan` itself was an `ID` type, the LLM interpreted this as a request for *all plan details* (i.e., `PlanOutput[]`). This was an attempt to provide a useful, comprehensive output given the ambiguity, but it **exposed internal representation** (`PlansDoc` fields) and conflicted with the principle of "Strict Representation Independence" established in prior concepts.

2. **Explicit Return Type Definitions are Crucial for Queries:**
   To enforce representation independence consistently, query specifications *must* precisely define the structure of their output. A clearer initial specification like "returns an array of Plan IDs belonging to user" would have prevented the initial over-exposure. The user's explicit prompt to "change getPlans to only return Plan IDs" successfully rectified this by enforcing `Array<{ plans: Plan }>`.

3. **Action Return Values for Confirmation:**
   The refined `ProgressTracking` concept (in your final prompt) demonstrates best practice by explicitly defining return values for actions like `createPlan`, `addAmount`, `removeAmount`, and `updateGoalStatus` (e.g., `currentAmount: Number`, `goalReachedFlag: boolean`). This allows external users to immediately confirm the direct effects of an action with minimal data exposure, rather than relying solely on separate queries to infer state changes.
